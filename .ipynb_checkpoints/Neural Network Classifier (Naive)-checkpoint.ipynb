{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model: Naive Neural Nets with TensorFlow    \n",
    "Following https://www.tensorflow.org/beta/tutorials/keras/feature_columns    \n",
    "A classification of structured data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip install -q tensorflow==2.0.0-beta1\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in sample data pre-transformed\n",
    "df = pd.read_csv('transformed_sample.csv')\n",
    "df.head()\n",
    "df.set_index('Unnamed: 0', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data types consistent before converting to tf data type \n",
    "# Detect if any columns have types other than float64\n",
    "a=[]\n",
    "b=[]\n",
    "for i in range(len(df.columns)):\n",
    "    for j in range(len(df)):\n",
    "        if type(df.iloc[j,i]) == str:\n",
    "            a.append(j)\n",
    "            b.append(i)\n",
    "            \n",
    "for i in a:\n",
    "    for j in b:\n",
    "        try:\n",
    "            df.iloc[i,j] = int(df.iloc[i,j])\n",
    "        except:\n",
    "            df.iloc[i,j] = 3000 #set a large number as rank for exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split into train, test and val sets\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('affiliation')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['0', '1', '2', '3', '4', '5', '6', 'afternoon', 'evening', 'morning', 'night', 'rank_yr', 'count', 'added', 'removed', 'whitespace', 'touched', '3scale-cli', '3scale-istio-adapter', '3scale-porta-go-client', '3scale_toolbox', 'BenchmarkDotNet', 'CentOS-Dockerfiles', 'CliCommandLineParser', 'ConfigJSR', 'EOL_openstack-puppet', 'Fedora-Dockerfiles', 'JBossMAT', 'JGroups', 'LicenseFinder', 'Node', 'PackageKit', 'Penlight-ffi', 'Resteasy', 'Solenopsis', 'WordPress', 'a-mq-migration', 'aCute', 'abakus', 'abrt', 'abrt-java-connector', 'account-reports', 'activemq', 'activemq-artemis', 'activerecord-jdbc-adapter', 'aeolus-configure', 'aeolus-website-old', 'aerogear-aerodoc-backend', 'aerogear-android-core', 'aerogear-digger', 'aerogear-ios-push-objc', 'aerogear-js', 'aerogear-js-builder', 'aerogear-js-sdk', 'aerogear-parent', 'aerogear-testing-tools', 'aerogear-unifiedpush-java-client', 'aerogear-unifiedpush-server', 'aerogear-unifiedpush-server-integration-tests', 'aerogear-webpush-server', 'aerogear-windows-push', 'aerogear-xamarin-sdk', 'aerogear.org', 'aesh', 'aesh-readline', 'aexpect', 'aki', 'akonadi', 'akonadi-calendar', 'akonadi-contacts', 'akonadi-google-applets', 'akonadi-next', 'akonadi-search', 'akonadi-vkontakte', 'akonadiconsole', 'akregator', 'alertmanager', 'amarok', 'anaconda', 'analitza', 'andmore', 'android-qt', 'android-qt-mobility', 'android-qt-ndk', 'android-qtwebkit', 'android-sdk-operator', 'android-showcase-template', 'angular-breadcrumb', 'angular-cli', 'angular-patternfly', 'angular-patternfly-demo-app', 'angular.js', 'ansible', 'ansible-1', 'ansible-container-demo', 'ansible-kubernetes-modules', 'ansible-kubevirt-modules', 'ansible-lint', 'ansible-modules-core', 'ansible-modules-extras', 'ansible-pulp3', 'ansible-role-httpd', 'ansible-role-tripleo-backup', 'ansible-role-tripleo-ci-reproducer', 'ansible-role-tripleo-image-build', 'ansible-role-weirdo-kolla', 'ansible-runner', 'ansible-runner-service', 'ansibullbot', 'api', 'apicast', 'apimachinery', 'apiman', 'apiman-1', 'apiman-cli', 'apiman-guides', 'apiman-quickstarts', 'apiman-test', 'apiserver', 'apisonator', 'apollo-android', 'apper', 'application-crd-client', 'archetypes', 'ark', 'arquillian', 'arquillian-algeron', 'arquillian-container-chameleon', 'arquillian-container-jbossas', 'arquillian-container-openejb', 'arquillian-container-tomcat', 'arquillian-container-undertow', 'arquillian-container-weld', 'arquillian-core', 'arquillian-cube', 'arquillian-daemon', 'arquillian-droidium', 'arquillian-extension-byteman', 'arquillian-extension-drone', 'arquillian-extension-jacoco', 'arquillian-extension-persistence', 'arquillian-extension-qunit', 'arquillian-extension-rest', 'arquillian-extension-spring', 'arquillian-extension-warp', 'arquillian-governor', 'arquillian-gradle-plugin', 'arquillian-graphene', 'arquillian-recorder', 'arquillian-rusheye', 'arquillian-spacelift', 'arquillian-spacelift-gradle', 'arquillian-universe-bom', 'arquillian_deprecated', 'artemis-hawtio', 'artifact-proxy-operator', 'artikulate-data', 'asknot-ng', 'assemblygen', 'astapor', 'atl', 'atomicapp', 'audit-kernel', 'audit-userspace', 'augeas', 'autoscaler', 'avahi', 'avocado', 'avocado-server', 'avocado-vt', 'awsThreeScale_Authorizer', 'awx', 'azure-testing', 'babe-qml', 'baloo', 'baloo-widgets', 'bangarang', 'bastion', 'beah', 'beaker', 'beaker-project.org', 'beetle-studio', 'birt', 'blacktie', 'blog', 'bluedevil', 'bluez-qt', 'bodega-server', 'bodhi', 'boms', 'booster-common', 'booth', 'bootstrap-community', 'boxgrinder-appliances', 'boxgrinder-build', 'boxgrinder-build-plugins', 'bpel', 'bpmn2-modeler', 'breeze', 'breeze-gtk', 'breeze-icons', 'brprint3d', 'btparser', 'buho', 'buildkit', 'buildship', 'byteman', 'cab-it', 'cakephp-ex', 'calligra', 'calligra-history', 'camel', 'candle-decompiler', 'candlepin', 'cantor', 'capedwarf-blue', 'capedwarf-green', 'capedwarf-jboss-as', 'capedwarf-modules', 'capedwarf-shared', 'capedwarf-testsuite', 'capedwarf.org', 'caracalla', 'catalog-static-deploy', 'catasb', 'cdi-api-bridge', 'cdi-operator', 'cdo', 'cdt-gdb-adapter', 'ceilometer', 'cervisia', 'ceylon-compiler', 'ceylon-js', 'ceylon-module-resolver', 'ceylon-runtime', 'ceylon.language', 'che', 'che-docs', 'che-ls-jdt', 'che-operator', 'che-theia-samples', 'che-theia-terminal-extension', 'chef-handler-foreman', 'chkconfig', 'choqok', 'ci-config', 'cinch', 'cinder', 'citellus', 'cli', 'cli-migrate', 'cli-netlite', 'cli-proton-ruby', 'client-go', 'cloud-provider-alibaba-cloud', 'cloud-provider-azure', 'cloud-provider-gcp', 'cloud-provider-openstack', 'clufter', 'cluster-api-provider-external', 'cluster-registry', 'cobbler', 'cockpit', 'cockpit-container', 'cockpit-design', 'cockpit-project', 'codeformatter', 'collectd_exporter', 'colord-kde', 'command-line-api', 'community', 'community-website', 'component-operator', 'components', 'conductor', 'conferences', 'config', 'configmapcontroller', 'confine', 'connectors', 'conquirere', 'console', 'container-compliance', 'container-pipeline-service', 'containerized-data-importer', 'content', 'continuous-enterprise-development', 'contour', 'contrib', 'coolstuff', 'cordova-showcase-template', 'core', 'core-sdk', 'coreclr', 'corefx', 'corefxlab', 'corert', 'corosync', 'corrosion', 'cpp-client', 'cpython', 'craft', 'craft-blueprints-kde', 'crane', 'csharplang', 'custodia', 'custodia.kubernetes', 'cxf', 'cygwin', 'dagre', 'dashboard', 'dashbuilder', 'dashbuilder-website', 'data-mapper', 'data-sync-server', 'data-sync-ui', 'dawnsci', 'dbs-server', 'demo-sonarqube', 'deploy-tools', 'descriptors', 'devassistant-addon', 'devel', 'developer-platform-install', 'developer.fedoraproject.org', 'device-plugin-manager', 'devnation-blog-source', 'devops-blog', 'digikam', 'digikam-doc', 'digikam-software-compilation', 'dirigible', 'discover', 'diskimage-builder', 'dist', 'ditto', 'django', 'django-ex', 'dltk.core', 'dltk.javascript', 'dltk.ruby', 'dltk.tcl', 'dmpy', 'dns', 'doc-guidelines', 'docfx', 'docker-addon', 'docker-gerrit', 'docker-gogs', 'docker-rpm', 'docker-swish', 'dockerui', 'docs', 'docs-web', 'docs.aerogear.org', 'docs.modcluster.io', 'documentation', 'docutils', 'dolphin', 'dolphin-plugins', 'dotnet', 'dotnet-docker-samples', 'dotnetfoundation-website', 'drift-backend', 'drools', 'dynflow', 'dyninst', 'e4.tools', 'eavp', 'ecf', 'eclemma', 'eclipse-collections', 'eclipse.jdt.core', 'eclipse.jdt.debug', 'eclipse.jdt.ls', 'eclipse.jdt.ui', 'eclipse.pde.ui', 'eclipse.platform.common', 'eclipse.platform.debug', 'eclipse.platform.releng', 'eclipse.platform.runtime', 'eclipse.platform.swt', 'eclipse.platform.swt.binaries', 'eclipse.platform.team', 'eclipse.platform.ua', 'eclipse.platform.ui', 'edk2', 'efibootmgr', 'efivar', 'egit', 'elisa', 'emerge-history', 'emf', 'emf.emfstore.core', 'enablement-docs', 'enhancements', 'enmasse', 'enmasse-example-clients', 'enmasse-rest', 'enmasseproject', 'epp.mpc', 'epp.packages', 'errai', 'errai-1', 'errai-tutorial', 'esb-message-admin', 'examples', 'external-storage', 'extra-cmake-modules', 'fabric8', 'fabric8-ci-seed', 'fabric8-devops', 'fabric8-forge', 'fabric8-installer', 'fabric8-ipaas', 'fabric8-jbpm-designer', 'fabric8-maven-plugin', 'fabric8-online', 'fabric8-online-docs', 'fabric8-pipeline-library', 'fabric8-pipeline-library-pre-versioning', 'fabric8-profiles', 'fabric8-test', 'faces', 'faf', 'falkon', 'fedbadges', 'federation', 'fedmsg', 'fedmsg_meta_fedora_infrastructure', 'fedora-college', 'fedora-openhw2012', 'fedora-packages', 'fedora-releng-dash', 'fedora-stable-config', 'fedora-stats-tools', 'fedora-tagger', 'fence-agents', 'fence-virt', 'firehose-saved-data', 'firewalld', 'flatpak-kde-applications', 'flatpak-platform-plugin', 'fluent-plugin-kubernetes_metadata_filter', 'fmn', 'fog', 'foreman', 'foreman-infra', 'foreman-installer-staypuft', 'foreman-packaging', 'foreman-selinux', 'foreman_abrt', 'foreman_ansible', 'foreman_bootdisk', 'foreman_deployments', 'foreman_discovery', 'foreman_fog_proxmox', 'foreman_maintain', 'foreman_openscap', 'foreman_pipeline', 'foreman_remote_execution', 'forklift', 'frakti', 'fsharp-api-docs', 'furnace', 'furnace-cdi', 'furnace-simple', 'fuse-quickstarts', 'fusor', 'fusor-installer', 'fusor-server', 'galaxy', 'galleon', 'galleon-plugins', 'gcompris', 'gcompris-data', 'gef', 'gef-legacy', 'gef3d', 'gemini.web.gemini-web-container', 'generator', 'generator-pfelement', 'gengo', 'gerrit-admin', 'gerrit-trigger-plugin', 'gevent-socketio', 'ginger', 'gingerbase', 'gitcontroller-1', 'github2fedmsg', 'glance', 'glance_store', 'gluon', 'gluster-kubernetes', 'gnome-abrt', 'gofabric8', 'gogs', 'goimprove', 'golang-container', 'golo-lang', 'gpgmepp', 'grafana-docker', 'granatier', 'graphite_exporter', 'grinder', 'gwenview', 'hammer-cli', 'hammer-cli-csv', 'hammer-cli-foreman', 'hammer-cli-import', 'haproxy_exporter', 'hawtio', 'hawtio-core', 'hawtio-core-dts', 'hawtio-git', 'hawtio-integration', 'hawtio-kubernetes', 'hawtio-oauth', 'heat', 'heketi', 'hexboard', 'hibernate-demos', 'hibernate-hql-parser', 'hibernate-jdocbook-style', 'hibernate-ogm', 'hibernate-ogm-cassandra', 'hibernate-ogm-couchdb', 'hibernate-ogm-ehcache', 'hibernate-ogm-ignite', 'hibernate-ogm-redis', 'hibernate-orm', 'hibernate-search', 'hibernate-search-6-poc', 'hibernate-shards', 'hibernate-tools', 'hibernate-validator', 'hibernate.org', 'hiredis-rb', 'hono', 'horizon', 'hornetq', 'host-inventory', 'httpd-container', 'hudson.clients.rest', 'hudson.core', 'hudson.plugins.legacy-maven', 'hudson.plugins.subversion', 'hyperconverged-cluster-operator', 'hyperkit', 'ibus', 'ice', 'idm-console-framework', 'ike-prow-plugins', 'image', 'imagefactory-console', 'immutant', 'immutant-release', 'immutant.org', 'incubator-ripple', 'index-fm', 'infinispan', 'infinispan-1', 'infinispan-cachestore-cassandra', 'infinispan-cachestore-cloud', 'infinispan-console-mockup', 'infinispan-hadoop', 'infinispan-management-console', 'infinispan-simple-tutorials', 'infinispan-spark', 'infispector', 'influxdb_exporter', 'infographic', 'infra-puppet', 'infrared', 'ingress-gce', 'ingress-nginx', 'initscripts', 'insights-advisor-frontend', 'insights-chrome', 'insights-client-role', 'insights-core', 'insights-core-assets', 'insights-dashboard', 'insights-deployment-test', 'insights-frontend', 'insights-frontend-assets', 'insights-frontend-components', 'insights-frontend-starter-app', 'insights-ocp-api', 'insights-ocp-controller', 'insights-rbac', 'insights-remediations-frontend', 'intellij-idea-plugin', 'international', 'ionic-showcase', 'ios-showcase-template', 'ipaas-quickstarts', 'ironic', 'jaeger', 'jaeger-kubernetes', 'january', 'january-forms', 'javaee-descriptors', 'javassist', 'jbdevstudio-devdoc', 'jbdevstudio-product', 'jbdevstudio-website', 'jberet-support', 'jberet-wildfly-samples', 'jboss-activemq-artemis', 'jboss-as', 'jboss-eap-archetypes', 'jboss-eap-boms', 'jboss-eap-quickstarts', 'jboss-ejb-client', 'jboss-jdg-quickstarts', 'jboss-logbridge', 'jboss-logging-tools', 'jboss-logmanager', 'jboss-marshalling', 'jboss-on-quickstarts', 'jboss-picketlink-quickstarts', 'jboss-portal-quickstarts', 'jboss-remoting', 'jboss-sandbox-quickstarts', 'jboss-stacks', 'jboss-wfk-archetypes', 'jboss-wfk-quickstarts', 'jbosstools-base', 'jbosstools-birt', 'jbosstools-bpel', 'jbosstools-build', 'jbosstools-build-ci', 'jbosstools-build-sites', 'jbosstools-central', 'jbosstools-deltacloud', 'jbosstools-discovery', 'jbosstools-documentation', 'jbosstools-download.jboss.org', 'jbosstools-esb', 'jbosstools-forge', 'jbosstools-freemarker', 'jbosstools-full-svn-mirror', 'jbosstools-fuse', 'jbosstools-fuse-extras', 'jbosstools-gwt', 'jbosstools-hibernate', 'jbosstools-install-grinder', 'jbosstools-integration-stack', 'jbosstools-integration-stack-tests', 'jbosstools-integration-tests', 'jbosstools-javaee', 'jbosstools-jbpm', 'jbosstools-jst', 'jbosstools-livereload', 'jbosstools-locus', 'jbosstools-openshift', 'jbosstools-playground', 'jbosstools-portlet', 'jbosstools-runtime-soa', 'jbosstools-server', 'jbosstools-target-platforms', 'jbosstools-versionwatch', 'jbosstools-vpe', 'jbosstools-webservices', 'jbosstools-website', 'jbossws-cxf', 'jbossws-hudson', 'jbpm', 'jbpm-designer', 'jcodings', 'jcr', 'jdf-stack', 'jenkins', 'jenkins-jnlp-client', 'jenkins-job-builder', 'jenkins-pipeline-library', 'jenkins-plugin', 'jenkins-sync-plugin', 'jenkinshift', 'jetty.project', 'jetty.toolchain', 'jetty.website', 'jgit', 'jiminy-html-server', 'jnosql', 'jnosql-aphrodite', 'jnosql-artemis', 'jnosql-artemis-extension', 'jnosql-diana-driver', 'jokre', 'joni', 'jose', 'jovie', 'jreadline', 'jruby', 'jruby-1', 'jruby-launcher', 'jruby-maven-plugins', 'jsmoke', 'json-schema', 'jsonstats', 'jsr352', 'jube', 'juicer', 'juk', 'jwt-redhat', 'k3b', 'k8s.io', 'kactivitymanagerd', 'kaddressbook', 'kaffeine', 'kafo', 'kalarm', 'kalarmcal', 'kalgebra', 'kalzium', 'kanagram', 'kannasaver', 'kansible', 'kapman', 'kapua', 'karaf', 'kate', 'katello-1', 'katello-cli', 'katello-host-tools', 'katello-installer', 'katello-installer-legacy', 'katello-misc', 'katello-website', 'katello.org', 'katello_api', 'katomic', 'kblocks', 'kcachegrind', 'kcalcore', 'kcalutils', 'kcontacts', 'kcoreaddons', 'kcwsh', 'kdb', 'kde-baseapps', 'kde-build-metadata', 'kde-ruleset', 'kde-runtime', 'kde-workspace', 'kdeclarative', 'kdeconnect-android', 'kdeconnect-kde', 'kdelibs', 'kdenlive', 'kdepim', 'kdepim-runtime', 'kdepimlibs', 'kdeplasma-addons', 'kdev-clang', 'kdev-css', 'kdev-embedded', 'kdev-go', 'kdev-php', 'kdev-python', 'kdev-qmake', 'kdev-qmljs', 'kdev-ruby', 'kdev-rust', 'kdev-upload', 'kdev-www', 'kdev-xtest', 'kdevelop', 'kdevelop-pg-qt', 'kdevplatform', 'kdewin', 'kdewin-installer', 'kdf', 'kdiagram', 'kdiff3', 'kdoctools', 'kdots', 'keditbookmarks', 'kexi', 'keycloak', 'keycloak-1', 'keycloak-documentation', 'keycloak-gatekeeper', 'keycloak-js-bower', 'keycloak-nodejs-admin-client', 'keycloak-nodejs-auth-utils', 'keycloak-operator', 'keycloak-quickstarts', 'keycloak-web', 'keyple-java', 'kfilemetadata', 'kfind', 'kgeography', 'kget', 'kglobalaccel', 'kgoldrunner', 'kgraphviewer', 'khaleesi', 'khelpcenter', 'khipu', 'kiali', 'kiali-client-python', 'kiali-design', 'kiali-ui', 'kiali.io', 'kibana', 'kigo', 'kile', 'kimchi', 'kimono', 'kimtoy', 'kinfocenter', 'kio', 'kio-extras', 'kio-upnp-ms', 'kipi-plugins', 'kirigami', 'kitemmodels', 'kiten', 'kldap', 'kleopatra', 'klimbgrades', 'kmail', 'kmail-account-wizard', 'kmailtransport', 'kmines', 'kmplayer', 'kmymoney', 'knavalbattle', 'knetwalk', 'knights', 'koffice', 'koffice-chartingshape', 'koffice-formulashape', 'koffice-plugins', 'kolf', 'kolourpaint', 'kommander', 'kompare', 'kompose', 'konqueror', 'konquest', 'konsole', 'konversation', 'kopete', 'kopete-smpppdcs', 'kops', 'korganizer', 'korundum', 'kpackage', 'kpatch', 'kpeople', 'kphotoalbum', 'kplayer', 'kproperty', 'krdc', 'krecipes', 'kregexpeditor', 'krename', 'krfb', 'krita', 'kronometer', 'kronosnet', 'krusader', 'kscd', 'kscreen', 'ksirk', 'kspaceduel', 'kst-plot', 'kstars', 'ksudoku', 'kte-collaborative', 'ktexteditor', 'ktorrent', 'ktouch', 'ktp-accounts-kcm', 'ktp-contact-list', 'ktp-desktop-applets', 'ktp-kde', 'ktp-kded-module', 'ktp-testlib', 'ktp-text-ui', 'kube', 'kube-aggregator', 'kube-deploy', 'kube-openapi', 'kube-state-metrics', 'kubectl', 'kubernetes', 'kubernetes-client', 'kubernetes-device-plugins', 'kubernetes-model', 'kubernetes-pipeline-plugin', 'kubernetes-plugin', 'kubespark-operator', 'kubevirt', 'kubevirt-1', 'kubevirt-ansible', 'kubevirt-operator', 'kubevirtci', 'kuksa.ide', 'kura', 'kuser', 'kwalletmanager', 'kwave', 'kwayland', 'kwebkitpart', 'kwin', 'kwindowsaddons', 'kwooty', 'kxstitch', 'labplot', 'lago', 'latte-dock', 'launcher-booster-catalog', 'ldap_fluff', 'learning', 'legacy-pki', 'legacy_content', 'lets-chat', 'libkcddb', 'libkcompactdisc', 'libkdcraw', 'libkdegames', 'libkeduvocdocument', 'libkexiv2', 'libkface', 'libkfbapi', 'libkgapi', 'libkgeomap', 'libkimageannotator', 'libkipi', 'libkscreen', 'libktorrent', 'liblikeback', 'libmediawiki', 'libqb', 'libqgit2', 'library', 'libreport', 'libstreamanalyzer', 'libstreams', 'libswid', 'libvirt', 'libvirt-cim', 'libvirt-console-proxy', 'libvirt-dbus', 'libvirt-glib', 'libvirt-go', 'libvirt-go-xml', 'libvirt-java', 'libvirt-ocaml', 'libvirt-php', 'libvirt-python', 'libvirt-rust', 'libvirt-sandbox', 'libvirt-tck', 'libvirt-test-API', 'libvirt-virshcmdref', 'libvirt-wiki', 'lightblue-applications', 'lightblue-client', 'lightblue-core', 'lightblue-dockerfiles', 'lightblue-java-generator', 'lightblue-migrator', 'lightblue-mongo', 'lightblue-puppet', 'lightblue-rdbms', 'lightblue-rest', 'linchpin', 'linux', 'linuxtools', 'linuxtools.eclipse-build', 'llilc', 'log-anomaly-detector', 'lokalize', 'lshw-tests', 'lua-resty-limit-traffic', 'lyo.core', 'lyo.rio', 'lyo.testsuite', 'm2e-apt', 'm2e-core', 'm2e-wtp-tests', 'm2e.wtp', 'machinelearning', 'machinelearning-samples', 'mail', 'managed-services', 'managed-services-broker', 'manila', 'marble', 'mauikit', 'maven-qstools-plugin', 'mavengem', 'mazer', 'mediawiki-example', 'memcached_exporter', 'messagelib', 'microprofile-config', 'microprofile-fault-tolerance', 'microprofile-health', 'microprofile-lra', 'microprofile-metrics', 'milo', 'minikube', 'minishift', 'ministro', 'miq-HP-OperationsManager', 'miq-ci', 'mirrormanager2', 'misc-bdwgc', 'misc-winlibs', 'moVirt', 'mobile-cli', 'mobile-core', 'mobile-crd-client', 'mobile-developer-console', 'mobile-security-service-operator', 'moby', 'mobywebsite', 'mock', 'mod_cluster', 'mod_proxy_cluster', 'modeshape', 'modeshape-examples', 'modeshape-performance', 'modeshape-rhq', 'modeshape-tools', 'mom', 'monarch', 'mongodb-container', 'mote', 'mrnet', 'mterm', 'muon', 'mylyn.docs', 'mylyn.reviews', 'mylyn.reviews.r4e', 'mysql-container', 'mysqld_exporter', 'nanook', 'narayana', 'narayana-spring-boot', 'narayana.io', 'nebula', 'nebula.widgets.nattable', 'necessitas-tools', 'nectar', 'neoscada', 'nepomuk-web-extractor', 'nepomuk-webminer', 'netbeans-plugin', 'netty', 'netty-1', 'netty-ant', 'netty-book', 'netty-website', 'networkmanagement', 'networkmanager-qt', 'neutron', 'nexus', 'nfs-ganesha', 'ng-grid', 'node-problem-detector', 'node-recovery', 'node_exporter', 'nomad-style', 'notifications-backend', 'nova', 'ntirpc', 'nulecule-library', 'nvd3', 'obal', 'objectteams', 'ocp-velero-plugin', 'ocs-apiserver', 'ocs-webserver', 'octario', 'ode', 'official-images', 'okteta', 'okular', 'omr', 'openj9', 'openj9-omr', 'openpracticelibrary', 'openscap', 'openscap-daemon', 'openshift-ansible', 'openshift-ansible-contrib', 'openshift-cartridge', 'openshift-docs', 'openshift-jenkins', 'openshift-jenkins-s2i-config', 'openshift-keycloak-cartridge', 'openshift-lightblue-cart', 'openshift-on-openstack', 'openshift-origin-cartridge-aerogear-push', 'openshift-quickstarts', 'openshift-scan', 'openshift-sonarqube', 'openshift-spark', 'openshift-status-cachet', 'openstack-packaging-doc', 'openstack-puppet-modules', 'openstack-selinux', 'opentracing-go', 'operator-lifecycle-manager', 'optaplanner', 'org', 'org_centos_cloud', 'orientation', 'origin', 'origin-web-catalog', 'origin-web-common', 'origin-web-console', 'origin-web-console-server', 'orion.client', 'orleans', 'osee', 'oshinko-cli', 'oshinko-rest', 'oshinko-s2i', 'oshinko-webui', 'ostree', 'ostree-go', 'osx-integration', 'otopi', 'overlay', 'ovirt-ansible-engine-setup', 'ovirt-docs', 'ovirt-dwh', 'ovirt-engine', 'ovirt-engine-api-metamodel', 'ovirt-engine-api-model', 'ovirt-engine-cli', 'ovirt-engine-extension-aaa-ldap', 'ovirt-engine-metrics', 'ovirt-engine-sdk', 'ovirt-engine-sdk-go', 'ovirt-engine-sdk-java', 'ovirt-guest-agent', 'ovirt-host-deploy', 'ovirt-hosted-engine-ha', 'ovirt-hosted-engine-setup', 'ovirt-imageio', 'ovirt-iso-uploader', 'ovirt-node', 'ovirt-node-ng', 'ovirt-openshift-extensions', 'ovirt-optimizer', 'ovirt-provider-ovn', 'ovirt-register', 'ovirt-reports', 'ovirt-site', 'ovirt-system-tests', 'ovirt-ui-components', 'ovirt-vdsmfake', 'ovirt-web-ui', 'ovirt_provision_plugin', 'ovs-cni', 'oxygen', 'oxygen-fonts', 'oxygen-gtk', 'oxygen-icons5', 'oz', 'pacemaker', 'packages-chr', 'packages-clib', 'packages-http', 'packages-ltx2htm', 'packages-semweb', 'packages-sgml', 'packages-ssl', 'packages-swipl-win', 'packages-xpce', 'packaging', 'paho.mqtt.cpp', 'palapeli', 'parfait', 'parley', 'patchutils', 'patternfly', 'patternfly-1', 'patternfly-accessibility-demo', 'patternfly-angular2-demo-app', 'patternfly-blog', 'patternfly-conference', 'patternfly-css', 'patternfly-demo-app', 'patternfly-elements', 'patternfly-google-code-prettify', 'patternfly-jquery', 'patternfly-next', 'patternfly-ng', 'patternfly-org', 'patternfly-react', 'patternfly-sass', 'patternfly-site', 'patternlab-node', 'pbench', 'pcp', 'pcp-webapp-vector', 'pcs', 'pdc-updater', 'pdt', 'perf-tests', 'performance', 'performancecopilot', 'perlkde', 'perlqt', 'persistence', 'pesign', 'phonon', 'phonon-mmf', 'phonon-mplayer', 'phonon-quicktime', 'phonon-waveout', 'phonon-xine', 'picketbox', 'picketbox-cdi', 'picketbox-container-stale', 'picketbox-quickstarts', 'picketbox5-deprecate', 'picketlink', 'picketlink-as-subsystem', 'picketlink-bindings', 'picketlink-certmgmt', 'picketlink-console', 'picketlink-idm', 'picketlink-idm-restored', 'picketlink-tests', 'picketlink-wfk-quickstarts', 'picmi', 'pisoni', 'pkgwat.api', 'pki', 'pki-wiki', 'plasma-desktop', 'plasma-framework', 'plasma-maliit-framework', 'plasma-mediacenter', 'plasma-mobile', 'plasma-mycroft', 'plasma-nm', 'plasma-pa', 'plasma-phone-components', 'plasma-sdk', 'plasma-settings', 'plasma-workspace', 'plasmate', 'plugin-arquillian', 'plweb', 'plweb-www', 'porta', 'postgresql-container', 'practice-library', 'prawn', 'presentations', 'pressgang-documentation-guide', 'pressgang-tools', 'print-manager', 'project-system', 'prom2json', 'prometheus', 'protostream', 'ptp', 'publictransport', 'publishing-bot', 'pubnub-go', 'pulp', 'pulp-ci', 'pulp-smash', 'pulp_deb', 'pulp_docker', 'pulp_ostree', 'pulp_puppet', 'pulp_rpm', 'pulpcore', 'pulpcore-plugin', 'pulpproject.org', 'pulseaudio-qt', 'puma', 'puppet-certs', 'puppet-dns', 'puppet-foreman_proxy', 'puppet-jboss_admin', 'puppet-katello', 'puppet-katello_devel', 'puppet-pulp', 'puppet-qpid', 'puppetlabs-apache', 'pure-bot', 'pushgateway', 'pyjose', 'python-glanceclient', 'python-gssapi', 'python-iniparse', 'python-openstackclient', 'python-rhsm', 'python-social-auth', 'pyudev', 'qe-tools', 'qemu', 'qqc2-desktop-style', 'qtcurve', 'qtruby', 'quanta', 'query-api', 'quickstart', 'quickstarts', 'qvto', 'qyoto', 'radanalyticsio', 'radargun', 'rails', 'rails-packaging', 'rally', 'rap', 'rap.incubator.gef', 'rdf4j', 'rdf4j-doc', 'rdf4j-storage', 'rdf4j-testsuite', 'rdf4j-tools', 'rdo-infra-playbooks', 'rdo-jobs', 'rdoinfo', 're-client', 're-core', 're-worker-ircnotify', 'reactive', 'recommenders', 'reddeer', 'redis-rb', 'redmine', 'redmine_backlogs', 'rekollect', 'rekonq', 'release', 'releng-tools', 'remoting', 'repoman', 'reportd', 'reports', 'resolver', 'resource-agents', 'resque', 'rest', 'restraint', 'retrace-server', 'review.rdoproject.org-config', 'rhc-ose', 'rhelements', 'rhiot', 'rhq', 'rhq-agent-plugin-plugin', 'rhscl-container-ci', 'rhscl-dockerfiles', 'richbeans', 'riftsaw', 'riftsaw-ode', 'ring-kde', 'rkward', 'roaster', 'rocs', 'rolekit', 'roslyn', 'roslyn-analyzers', 'roslyn-api-docs', 'router-docker-base', 'rt.equinox.p2', 'ruby-libvirt', 'ruby-newt', 'rubygems', 'rubysl-socket', 'rubyspec', 'runcible', 's2i-php-container', 's2i-python-container', 's2i-ruby-container', 'saas-analytics', 'saasherder', 'sahara', 'sample-apiserver', 'satyr', 'scaffold-html5', 'scalable_tools_workshop', 'scanning', 'scap-workbench', 'sce-community-content', 'scl-utils', 'sclo-ci-tests', 'scout.rt', 'seam-forge', 'security', 'selinux', 'selinux-kernel', 'selinux-policy', 'selinux-policy-contrib', 'sensor-camera', 'service-broker-ci', 'servicemix', 'setroubleshoot', 'share-like-connect', 'shim', 'shrinkwrap', 'shumgrepper', 'sig-core-t_functional', 'sig-core-t_ltp', 'silk', 'simon', 'sink', 'sisu', 'sisu.inject', 'site-windup.jboss.org', 'skalli', 'sketchpod', 'skopeo', 'slf4j', 'slush-hawtio-typescript', 'smallfile', 'smart-proxy', 'smart-testing', 'smart_proxy_chef', 'smart_proxy_dynflow', 'smart_proxy_vault', 'smarthome', 'smokekde', 'smokeqt', 'snorenotify', 'snowdrop-cloud-devex', 'soa-migration', 'solder', 'sonar-auth-openshift', 'soprano', 'sos', 'spacewalk', 'spacewalk-wiki', 'spacewalkproject', 'spacewalkproject-1', 'spark', 'speedmetal', 'spi', 'spring', 'spring-boot-circuit-breaker-booster', 'spring-boot-http-secured-booster', 'standard', 'statsd_exporter', 'statusfpo', 'staypuft', 'step', 'storage', 'strigi', 'strigiclient', 'strigiutils', 'strimzi', 'stripe-go', 'subscription-manager', 'sumo', 'superkaramba', 'supybot-fedmsg', 'swagger-ui', 'swift', 'swipl', 'swipl-devel', 'swish', 'switchyard', 'swtbot', 'syndesis', 'syndesis-documentation', 'syndesis-extensions', 'syndesis-integration-runtime', 'syndesis-jenkins', 'syndesis-qe', 'syndesis-react-poc', 'syndesis-rest', 'syndesis-system-tests', 'syndesis-ui', 'syndesis-ux', 'syndesis.io', 'syslog-ng', 'systemsettings', 'tag-it', 'tahrir', 'tahrir-api', 'team-operator', 'teiid', 'teiid-dashboard', 'teiid-komodo', 'teiid-olingo-odata4', 'teiid-spring-boot', 'teiid-thorntail', 'teiid-wildfly-quickstarts', 'teiid.io', 'telegram-qt', 'temp-maven-repo', 'tempest', 'tempest-plugin', 'tennera', 'tensorflow-wheels', 'test-infra', 'tests-foreman_openscap', 'testsuite', 'theforeman.org', 'theia', 'thermostat-container', 'thumbslug', 'thym', 'ticket-monster', 'ticketutil', 'titan.EclipsePlug-ins', 'tito', 'tlsfuzzer', 'tm4e', 'tmgmt_zanata', 'token-rp', 'tomcat-spring-boot-qualification', 'tool_belt', 'tools', 'toolset', 'torquebox', 'torquebox-lite', 'torquebox-openshift-express', 'torquebox-release', 'torquebox.org', 'transtats', 'tripleo-docs', 'tripleo-heat-templates', 'tripleo-image-elements', 'tripleo-incubator', 'tripleo-quickstart-utils', 'triquetrum', 'trunk.rdoproject.org', 'tsdb', 'tuned', 'tupi', 'tuskar', 'tuskar-ui', 'tutorial', 'udisks', 'umbrello', 'unified-push-helloworld', 'uomo', 'ups-config-operator', 'ups-operator-poc', 'user-guide', 'util-linux', 'utils', 'v2v-conversion-host', 'vdsm', 'vdsm-jsonrpc-java', 'velero', 'veritas', 'vert-x3', 'vertx-amqp-service', 'vertx-auth', 'vertx-by-example', 'vertx-camel-bridge', 'vertx-cassandra-client', 'vertx-config', 'vertx-consul-client', 'vertx-examples', 'vertx-hawkular-metrics', 'vertx-ignite', 'vertx-jdbc-client', 'vertx-jgroups', 'vertx-lang-scala', 'vertx-mail-client', 'vertx-maven-plugin', 'vertx-mongo-client', 'vertx-mysql-postgresql-client', 'vertx-reactive-streams', 'vertx-rx', 'vertx-service-discovery', 'vertx-stack', 'vertx-starter', 'vertx-stomp', 'vertx-web', 'vertx-web-site', 'vessel', 'virgo', 'virgo.apps', 'virgo.eclipse-mirror', 'virgo.ide', 'virgo.nano', 'virgo.packaging', 'virgo.sample-formtags', 'virgo.util', 'virgo.virgo-build', 'virt-manager', 'visual', 'vjet.all', 'vjet.core', 'vjet.dltk', 'vjet.eclipse', 'vjet.jsdt.parser', 'vjet.vsf', 'vmaas', 'vmctl', 'voyager-server', 'vpnkit', 'vulnerability-engine', 'vulnerability-ui', 'vvave', 'warbler', 'web-editor', 'web-ui', 'web-ui-components', 'web-ui-design', 'web-ui-operator', 'website', 'website-data', 'webtools.common', 'webtools.javaee', 'webtools.jsf', 'webtools.webservices', 'weirdo', 'weld-vertx', 'whitebox-tempest-plugin', 'widgetastic.core', 'wikitolearn-frontend', 'wikitolearn-sdk', 'wildfly', 'wildfly-archetypes', 'wildfly-arquillian', 'wildfly-core', 'wildfly-elytron', 'wildfly-maven-plugin', 'wildfly-naming-client', 'wildfly-openssl', 'wildfly-server-migration', 'wildfly-swarm.io', 'wildfly.org', 'wildflysite', 'wildwebdeveloper', 'windup', 'windup-distribution', 'windup-documentation', 'windup-eclipse-plugin', 'windup-fernflower', 'windup-legacy', 'windup-maven-plugin', 'windup-procyon', 'windup-quickstarts', 'windup-rulesets', 'windup-rulesets-server', 'windup-sample-apps', 'windup-web', 'windup-web-distribution', 'winery', 'winforms', 'wok', 'workshop', 'workshop-notebook', 'wpf', 'wrapanapi', 'www.jboss.org', 'xcode-install', 'xdg-desktop-portal-kde', 'xnio', 'xsemantics', 'xtext', 'xtext-core', 'xtext-eclipse', 'xtext-extras', 'xtext-idea', 'xtext-xtend', 'yangster', 'zanata', 'zanata-api', 'zanata-assets', 'zanata-client', 'zanata-common', 'zanata-mt', 'zanata-platform', 'zanata-python-client', 'zanata-scripts', 'zanata-server', 'zanata-spa', 'zanata-styleguide', 'zanata-tests', 'zanshin']\n",
      "A batch of afternoon: tf.Tensor([0.  0.5 0.  0.5 0. ], shape=(5,), dtype=float64)\n",
      "A batch of affiliation: tf.Tensor([2. 0. 0. 1. 0.], shape=(5,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of afternoon:', feature_batch['afternoon'])\n",
    "  print('A batch of affiliation:', label_batch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_ds))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a feature column\n",
    "# and to transform a batch of data\n",
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.]\n",
      " [ 2.]\n",
      " [ 6.]\n",
      " [13.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "rank_yr = feature_column.numeric_column(\"rank_yr\")\n",
    "demo(rank_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append feature columns to create layer\n",
    "feature_columns = []\n",
    "for header in df.columns:\n",
    "    if header == 'affiliation':\n",
    "        next\n",
    "    else:\n",
    "        feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "51/51 [==============================] - 65s 1s/step - loss: nan - accuracy: 0.3840 - val_loss: nan - val_accuracy: 0.3697\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 47s 929ms/step - loss: nan - accuracy: 0.3773 - val_loss: nan - val_accuracy: 0.3697\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 48s 940ms/step - loss: nan - accuracy: 0.3773 - val_loss: nan - val_accuracy: 0.3697\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 47s 926ms/step - loss: nan - accuracy: 0.3773 - val_loss: nan - val_accuracy: 0.3697\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 47s 924ms/step - loss: nan - accuracy: 0.3773 - val_loss: nan - val_accuracy: 0.3697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcd28209cc0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try sigmoid as output layer activation function\n",
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True)\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 15:31:28.768005 140522796001088 deprecation.py:323] From /opt/app-root/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 62s 1s/step - loss: nan - accuracy: 0.3912 - val_loss: nan - val_accuracy: 0.3697\n",
      "Epoch 2/6\n",
      "51/51 [==============================] - 45s 887ms/step - loss: nan - accuracy: 0.3773 - val_loss: nan - val_accuracy: 0.3697\n",
      "Epoch 3/6\n",
      "51/51 [==============================] - 45s 883ms/step - loss: nan - accuracy: 0.3773 - val_loss: nan - val_accuracy: 0.3697\n",
      "Epoch 4/6\n",
      "51/51 [==============================] - 47s 931ms/step - loss: nan - accuracy: 0.3773 - val_loss: nan - val_accuracy: 0.3697\n",
      "Epoch 5/6\n",
      "51/51 [==============================] - 45s 889ms/step - loss: nan - accuracy: 0.3773 - val_loss: nan - val_accuracy: 0.3697\n",
      "Epoch 6/6\n",
      "51/51 [==============================] - 46s 903ms/step - loss: nan - accuracy: 0.3773 - val_loss: nan - val_accuracy: 0.3697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcd281f0a58>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try sigmoid as output layer activation function\n",
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(1, activation='tanh')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True)\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network \n",
    "https://github.com/rionaldichandraseta/gan-on-structured-data/blob/master/gan/notebooks/gan-toy.ipynb    \n",
    "The idea is that we only extract the data labeled as RH. With them, the modle generates a generators, which is used to generate fake observations. The model is trained to distinguish fake observations from true dataset. The result from this model is the conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling tensorflow-2.0.0b1:\n",
      "  Would remove:\n",
      "    /opt/app-root/bin/freeze_graph\n",
      "    /opt/app-root/bin/saved_model_cli\n",
      "    /opt/app-root/bin/tensorboard\n",
      "    /opt/app-root/bin/tf_upgrade_v2\n",
      "    /opt/app-root/bin/tflite_convert\n",
      "    /opt/app-root/bin/toco\n",
      "    /opt/app-root/bin/toco_from_protos\n",
      "    /opt/app-root/lib/python3.6/site-packages/tensorflow-2.0.0b1.dist-info/*\n",
      "    /opt/app-root/lib/python3.6/site-packages/tensorflow/*\n",
      "Proceed (y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "Uninstalling Keras-2.2.4:\n",
      "  Would remove:\n",
      "    /opt/app-root/lib/python3.6/site-packages/Keras-2.2.4.dist-info/*\n",
      "    /opt/app-root/lib/python3.6/site-packages/docs/*\n",
      "    /opt/app-root/lib/python3.6/site-packages/keras/*\n",
      "Proceed (y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Experimented with trainning code but there are problem with module versiosn\n",
    "#Uninstall module first and reinstall tf and keras of certain versions\n",
    "#Do this in terminal\n",
    "!pip uninstall tensorflow\n",
    "!pip uninstall keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/app-root/lib/python3.6/site-packages (1.14.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (0.33.4)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (0.1.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.22.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.16.4)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: h5py in /opt/app-root/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/app-root/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.15.4)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting keras==2.0.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/5a/c7fe49396e9d14f89b05ea550f3d5f8c4cfb3af20e831a3d323fec2a0d5b/Keras-2.0.9-py2.py3-none-any.whl (299kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 2.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /opt/app-root/lib/python3.6/site-packages (from keras==2.0.9) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/app-root/lib/python3.6/site-packages (from keras==2.0.9) (1.16.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/app-root/lib/python3.6/site-packages (from keras==2.0.9) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /opt/app-root/lib/python3.6/site-packages (from keras==2.0.9) (5.1.1)\n",
      "Installing collected packages: keras\n",
      "  Found existing installation: Keras 2.2.4\n",
      "    Uninstalling Keras-2.2.4:\n",
      "      Successfully uninstalled Keras-2.2.4\n",
      "Successfully installed keras-2.0.9\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras==2.0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras import backend as k\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.merge import concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Load the dataset\n",
    "        df_gan = df[df['affiliation']==1] #Filter out only those identified as RHer for GAN\n",
    "        # Split into train, test and val sets\n",
    "        train, test = train_test_split(df_gan, test_size=0.2)\n",
    "        y_train = train['affiliation']\n",
    "        x_train = train.loc[:, df_gan.columns != 'affiliation']\n",
    "        y_train = y_train.astype(object)\n",
    "        \n",
    "        self.img_rows = len(x_train)\n",
    "        self.img_cols = len(x_train.columns)\n",
    "        self.img_shape = (self.img_rows, self.img_cols)\n",
    "        self.latent_dim = 100 \n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=64, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        df_gan = df[df['affiliation']==1] #Filter out only those identified as RHer for GAN\n",
    "        # Split into train, test and val sets\n",
    "        train, test = train_test_split(df_gan, test_size=0.2)\n",
    "        y_train = train['affiliation']\n",
    "        x_train = train.loc[:, df_gan.columns != 'affiliation']\n",
    "        y_train = y_train.astype(object)\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            imgs = x_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=30000, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/app-root/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/app-root/lib/python3.6/site-packages (from keras) (1.3.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/app-root/lib/python3.6/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/app-root/lib/python3.6/site-packages (from keras) (1.16.4)\n",
      "Requirement already satisfied: h5py in /opt/app-root/lib/python3.6/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/app-root/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in /opt/app-root/lib/python3.6/site-packages (from keras) (5.1.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/app-root/lib/python3.6/site-packages (from keras) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
      "\u001b[K     |████████████████████████████████| 109.2MB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 7.9MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.12.0)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/5d/b434403adb2db8853a97828d3d19f2032e79d630e0d11a8e95d243103a11/grpcio-1.22.0-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 8.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (0.33.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.16.4)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "\u001b[K     |████████████████████████████████| 491kB 8.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/app-root/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 8.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 67.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: h5py in /opt/app-root/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 66.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /opt/app-root/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.15.4)\n",
      "Building wheels for collected packages: absl-py, gast, termcolor\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-jqgm5o21/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-jqgm5o21/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-jqgm5o21/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built absl-py gast termcolor\n",
      "Installing collected packages: absl-py, astor, grpcio, tensorflow-estimator, gast, markdown, tensorboard, google-pasta, termcolor, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.22.0 markdown-3.1.1 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras \n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_df = pd.read_csv('gan_data.csv')\n",
    "    \n",
    "# Split into train, test and val sets\n",
    "train, test = train_test_split(df_gan, test_size=0.2)\n",
    "y_train = train['affiliation']\n",
    "x_train = train.loc[:, df_gan.columns != 'affiliation']\n",
    "y_train = y_train.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 1471064)           0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 512)               753185280 \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 753,316,865\n",
      "Trainable params: 753,316,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_59 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1471064)           1507840600\n",
      "_________________________________________________________________\n",
      "reshape_8 (Reshape)          (None, 872, 1687)         0         \n",
      "=================================================================\n",
      "Total params: 1,508,530,520\n",
      "Trainable params: 1,508,526,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class GAN():\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.img_rows = len(x_train)\n",
    "        self.img_cols = len(x_train.columns)\n",
    "        self.img_shape = (self.img_rows, self.img_cols)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "        \n",
    "\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            imgs = x_train.sample(batch_size)\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=20, batch_size=32, sample_interval=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
